import anthropic
from core.base import LLM
import yaml
import os
import requests

class Claude(LLM):
    """
    An abstract class representing a Claude model from Anthropic.

    Attributes:
        _CLIENT (anthropic.Anthropic): The client object for the Anthropic API.
        RESPONSE_FORMAT (str): The format of the response from the API. Not used in Claude models.
        _PROMPTS (dict): The prompts for the model.
    """

    def __init__(
        self, randomly_flip_options: bool = False, shuffle_answer_options: bool = False
    ):
        super().__init__(
            randomly_flip_options=randomly_flip_options,
            shuffle_answer_options=shuffle_answer_options,
        )
        if "ANTHROPIC_API_KEY" not in os.environ:
            raise ValueError(
                "Cannot access Anthropic API due to missing API key. Please store your API key in environment variable 'ANTHROPIC_API_KEY'."
            )
        self._CLIENT = anthropic.Anthropic()
        self.RESPONSE_FORMAT = None
        with open("./models/Anthropic/prompts.yml") as f:
            self._PROMPTS = yaml.safe_load(f)

    def prompt(self, prompt: str, temperature: float = 0.0, seed: int = 42) -> str:
        """
        Generates a response to the provided prompt.

        Args:
            prompt (str): The prompt to generate a response for.
            temperature (float): The temperature value of the LLM. For strictly decision models, we use a temperature of 0.0.
            seed (int): The seed for controlling the LLM's output. It is not used in Claude models.

        Returns:
            str: The response generated by the LLM.
        """
        # Call the chat completions API endpoint
        response = self._CLIENT.messages.create(
            model=self.NAME,
            # we use a max_tokens (required parameter) of 1024 exactly as in the Anthropic API documentation
            max_tokens=1024,
            temperature=temperature,
            messages=[{"role": "user", "content": prompt}],
        )

        # Extract and return the answer
        return response.content[0].text
    

class ClaudeThreeFiveSonnet(LLM):
    """
    An abstract class representing a Claude 3.5 Sonnet model by Anthropic from ML/API API.

    Attributes:
        RESPONSE_FORMAT (str): The format of the response from the API. Not used in Claude models.
        _PROMPTS (dict): The prompts for the model.
    """
    def __init__(
        self, randomly_flip_options: bool = False, shuffle_answer_options: bool = False
    ):
        super().__init__(
            randomly_flip_options=randomly_flip_options,
            shuffle_answer_options=shuffle_answer_options,
        )
        if "AIML_API_KEY" not in os.environ:
            raise ValueError(
                "Cannot access AI/ML API due to missing API key. Please store your API key in environment variable 'AIML_API_KEY'."
            )
        self.RESPONSE_FORMAT = None
        with open("./models/Anthropic/prompts.yml") as f:
            self._PROMPTS = yaml.safe_load(f)
        self.NAME = "claude-3-5-sonnet-20240620"

    def prompt(self, prompt: str, temperature: float = 0.0, seed: int = 42) -> str:
        """
        Generates a response to the provided prompt.

        Args:
            prompt (str): The prompt to generate a response for.
            temperature (float): The temperature value of the LLM. For strictly decision models, we use a temperature of 0.0.
            seed (int): The seed for controlling the LLM's output. It is not used in Claude models.

        Returns:
            str: The response generated by the LLM.
        """
        # Call the chat completions API endpoint
        url = "https://api.aimlapi.com/messages"
        headers = {
            "Authorization": os.environ["AIML_API_KEY"],
            "Content-Type": "application/json"
        }    
        payload = {
            "model": self.NAME,
            "max_tokens": 1024,
            "temperature": temperature,
            "messages": [{"role": "user", "content": prompt}]
        }
        response = requests.post(url, headers=headers, json=payload)

        # TODO: Extract and return the answer
        return response


# Sonnet is implemented via ML/AI API above
# class ClaudeThreeFiveSonnet(Claude):
#     """
#     A class representing a Claude 3.5 Sonnet LLM that decides on the test cases provided.

#     Attributes:
#         NAME (str): The name of the model.
#     """

#     def __init__(
#         self, randomly_flip_options: bool = False, shuffle_answer_options: bool = False
#     ):
#         super().__init__(
#             randomly_flip_options=randomly_flip_options,
#             shuffle_answer_options=shuffle_answer_options,
#         )
#         self.NAME = "claude-3-5-sonnet-20240620"
        

class ClaudeThreeHaiku(Claude):
    """
    A class representing a Claude 3 Haiku LLM that decides on the test cases provided.

    Attributes:
        NAME (str): The name of the model.
    """

    def __init__(
        self, randomly_flip_options: bool = False, shuffle_answer_options: bool = False
    ):
        super().__init__(
            randomly_flip_options=randomly_flip_options,
            shuffle_answer_options=shuffle_answer_options,
        )
        self.NAME = "claude-3-haiku-20240307"
