from tests import Template
from base import LLM
from openai import OpenAI
import json
import yaml


class GPT(LLM):
    """
    An abstract class representing a GPT-based LLM from OpenAI.

    Attributes:
        NAME (str): The name of the model.
        _CLIENT (OpenAI): The OpenAI client.
        RESPONSE_FORMAT (str): The response format for the model. 
        _PROMPTS (dict): A dictionary containing the system and user prompts.
    """

    def __init__(self, randomly_flip_options: bool = False, shuffle_answer_options: bool = False):
        super().__init__(randomly_flip_options=randomly_flip_options, shuffle_answer_options=shuffle_answer_options)
        self._CLIENT = OpenAI()
        self.RESPONSE_FORMAT = "json_object"
        with open("./models/OpenAI/prompts.yml") as f:
            self._PROMPTS = yaml.safe_load(f)

    def prompt(self, prompt: str, temperature: float = 0.7, seed: int = 42) -> str:
        """
        Generates a response to the provided prompt.
        
        Args:
            prompt (str): The prompt to generate a response for.
            temperature (float): The temperature value of the LLM.
            seed (int): The seed for controlling the LLM's output.
        
        Returns:
            str: The response generated by the LLM.
        """
        # Call the chat completions API endpoint
        response = self._CLIENT.chat.completions.create(
            model=self.NAME,
            temperature=temperature,
            seed=seed,
            messages=[{"role": "user", "content": prompt}]
        )

        # Extract and return the answer
        return response.choices[0].message.content

    def populate(self, control: Template, treatment: Template, scenario: str, temperature: float = 0.7, seed: int = 42) -> tuple[Template, Template]:
        """
        Populates the blanks in the provided control and treatment templates according to the scenario.
        
        Args:
            control (Template): The control template to populate.
            treatment (Template): The treatment template to populate.
            scenario (str): A string describing the scenario for the population.
            temperature (float): The temperature value of the LLM.
            seed (int): The seed for controlling the LLM's output.
            
        Returns:
            tuple[Template, Template]: The populated control and treatment Template objects.
        """
        # 1. Populate the gaps in the control template based on the scenario
        if control is not None and len(control.get_gaps()) > 0:
            control = self._populate(control, scenario, temperature=temperature, seed=seed)

        # 2. Fill the gaps in the treatment template that are shared with the control template
        if control is not None and treatment is not None:
            insertions = control.get_insertions(origin='model')
            treatment.insert(insertions=insertions)

        # 3. Populate the remaining gaps in the treatment template based on the scenario
        if treatment is not None and len(treatment.get_gaps()) > 0:
            treatment = self._populate(treatment, scenario, temperature=temperature, seed=seed)

        return control, treatment

    def _define_response_format(self, mode: str, gaps: list[str]) -> object:
        """
        Defines the response format for controlled model outputs (special feature supported by OpenAI models).
        See https://openai.com/index/introducing-structured-outputs-in-the-api/

        Args:
            mode (str): The mode for controlling outputs, either "json_schema" (for Structured Outputs) or "json_object" (for JSON Mode).
            gaps (list[str]): A list of gaps in the template that need to be filled.

        Returns:
            object: An object describing the response format.
        """
        
        # Compile the response format for controlled model outputs (special feature supported by OpenAI models)
        if mode == "json_schema":
            # Works with gpt-4o-2024-08-06 and gpt-4o-mini-2024-07-18, see https://openai.com/index/introducing-structured-outputs-in-the-api/
            response_format = {
                "type": "json_schema",
                "json_schema": {
                    "name": "population_response",
                    "strict": True,
                    "schema": {
                        "type": "object",
                        "properties": {gap: {"type": "string"} for gap in gaps},
                        "required": gaps,
                        "additionalProperties": False
                    }
                }
            }
        elif mode == "json_object":
            # Enable OpenAI's JSON model for models that do support it but don't support structured output (i.e., json_schema)
            response_format = {
                "type": "json_object"
            }
        else:
            response_format = None

        return response_format

    def _populate(self, template: Template, scenario: str, temperature: float = 0.7, seed: int = 42) -> Template:
        """
        Populates the blanks in the provided template according to the scenario.
        
        Args:
            template (Template): The template to populate.
            scenario (str): A string describing the scenario/context for the population.
            temperature (float): The temperature value of the LLM.
            seed (int): The seed for controlling the LLM's output.

        Returns:
            Template: The populated Template object.
        """

        # Load the system and user prompts
        system_prompt = self._PROMPTS['system_prompt']
        user_prompt = self._PROMPTS['population_prompt']

        # Compile the format instructions (JSON format) based on the remaining gaps in the template
        gaps = template.get_gaps(origin='model')
        gaps_format = [f"    \"{gap}\": \"...\"" for gap in gaps]
        expected_format = "{\n" + ',\n'.join(gaps_format) + "\n}"

        # Compile the response format for controlled model outputs (special feature supported by OpenAI models)
        response_format = self._define_response_format(self.RESPONSE_FORMAT, gaps)

        # Insert the scenario, template, and format instructions into the prompt
        user_prompt = user_prompt.replace("{{scenario}}", scenario)
        user_prompt = user_prompt.replace("{{template}}", template.format())
        user_prompt = user_prompt.replace("{{format}}", expected_format)

        # Obtain a response from the LLM
        response = self._CLIENT.chat.completions.create(
            model=self.NAME,
            temperature=temperature,
            seed=seed,
            response_format=response_format,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ]
        )

        # Parse the insertions generated by the LLM and fix them, if needed (adding missing double square brackets, which the LLM sometimes forgets)
        insertions = json.loads(response.choices[0].message.content)
        insertions = self._fix_insertions(insertions)

        # Validate the insertions
        self._validate_population(template, insertions, response.choices[0].message.content)

        # Make the insertions in the template
        for pattern in insertions.keys():
            template.insert(pattern, insertions[pattern], 'model')

        return template

    def _fix_insertions(self, insertions: dict) -> dict:
        """
        Ensures all keys in the dictionary start with [[ and end with ]].
        
        Args:
            insertions (dict): The dictionary to format keys for.

        Returns:
            dict: A new dictionary with formatted keys.
        """

        fixed_insertions = {}

        for key, value in insertions.items():
            # Check if the key already starts with [[ and ends with ]]
            if not key.startswith('[['):
                key = '[[' + key
            if not key.endswith(']]'):
                key = key + ']]'
            
            # Add the formatted key to the new dictionary
            fixed_insertions[key] = value
        
        return fixed_insertions


class GptThreePointFiveTurbo(GPT):
    """
    A class representing a GPT-3.5-Turbo LLM that populates test cases according to the scenario 
    starting from the brackets that are either identical for both control and treatment or unique for control, 
    and then adding those unique for treatment.

    Attributes:
        NAME (str): The name of the model.
    """

    def __init__(self, randomly_flip_options: bool = False, shuffle_answer_options: bool = False):
        super().__init__(randomly_flip_options=randomly_flip_options, shuffle_answer_options=shuffle_answer_options)
        self.NAME = "gpt-3.5-turbo-0125"


class GptFourO(GPT):
    """
    A class representing a GPT-4o LLM that populates test cases according to the scenario 
    starting from the brackets that are either identical for both control and treatment or unique for control, 
    and then adding those unique for treatment.

    Attributes:
        NAME (str): The name of the model.
        RESPONSE_FORMAT (str): The response format for the model.
    """

    def __init__(self, randomly_flip_options: bool = False, shuffle_answer_options: bool = False):
        super().__init__(randomly_flip_options=randomly_flip_options, shuffle_answer_options=shuffle_answer_options)
        self.NAME = "gpt-4o-2024-08-06"
        self.RESPONSE_FORMAT = "json_schema"