from abc import ABC, abstractmethod
from tests import TestCase, Template, TestConfig, DecisionResult
import random


class PopulationError(Exception):
    """A class for exceptions raised during the population of test cases."""
    def __init__(self, message: str, template: Template = None, model_output: str = None):
        extended_message = message
        if template is not None:
            try:
                extended_message += f"\n\n--- TEMPLATE ---\n{template}"
            except:
                pass
        if model_output is not None:
            extended_message += f"\n\n--- MODEL OUTPUT ---\n{model_output}"

        super().__init__(extended_message)   
        self.message = message
        self.template = template
        self.model_output = model_output


class DecisionError(Exception):
    """A class for exceptions raised during the decision of test cases."""
    pass


class MetricCalculationError(Exception):
    """A class for exceptions raised during the calculation of metric for a given bias."""
    pass


class LLM(ABC):
    """
    Abstract base class representing a Large Language Model (LLM) capable of generating and performing cognitive bias test cases.
    
    Attributes:
        NAME (str): The name of the model.
        shuffle_answer_options (bool): Whether or not answer options shall be randomly shuffled when making a decision.
    """

    def __init__(self, shuffle_answer_options: bool = False):
        self.NAME = "llm-abstract-base-class"
        self.shuffle_answer_options = shuffle_answer_options

    @abstractmethod
    def prompt(self, prompt: str, temperature: float = 0.0, seed: int = 42) -> str:
        """
        Prompts the LLM with a text input and returns the LLM's answer.

        Args:
            prompt (str): The input prompt text.
            temperature (float): The temperature value of the LLM.
            seed (int): The seed for controlling the LLM's output.

        Returns:
            str: The LLM's answer to the input prompt.
        """
        pass

    @abstractmethod
    def populate(self, control: Template, treatment: Template, scenario: str, temperature: float = 0.0, seed: int = 42) -> tuple[Template, Template]:
        """
        Populates given control and treatment templates based on the provided scenario.

        Args:
            control (Template): The control template that shall be populated.
            treatment (Template): The treatment template that shall be populated.
            scenario (str): A string describing the scenario/context for the population.
            temperature (float): The temperature value of the LLM.
            seed (int): The seed for controlling the LLM's output.

        Returns:
            tuple[Template, Template]: The populated control and treatment templates.
        """
        pass

    @abstractmethod
    def decide(self, test_case: TestCase, temperature: float = 0.0, seed: int = 42) -> DecisionResult:
        """
        Makes the decisions defined in the provided test case (i.e., typically chooses one option from the control template and one option from the treatment template).

        Args:
            test_case (TestCase): The TestCase object defining the tests/decisions to be made.
            temperature (float): The temperature value of the LLM.
            seed (int): The seed for controlling the LLM's output.

        Returns:
            DecisionResult: A DecisionResult representing the decisions made by the LLM.
        """
        pass

    def _validate_population(self, template: Template, insertions: dict, raw_model_output: str = None) -> bool:
        """
        Validates if a model's generated insertions are valid for the provided template.

        Args:
            template (Template): The Template object for which insertions were generated.
            insertions (dict): A dictionary with all insertions that were generated by the model. Keys should be the patterns/gap instructions and values should be the generated insertions.
            raw_model_output (str): The raw model output. This is used for failure diagnostics in case the validation is unsuccessful.

        Returns:
            bool: True if the validation was successful. Otherwise, a PopulationError is raised.
        """

        # Get the remaining gaps from the template
        gaps = template.get_gaps()

        # Verify that insertions were generated for all remaining gaps
        for gap in gaps:
            if gap not in insertions:
                raise PopulationError(f"The gap '{gap}' has not been filled.", template, raw_model_output)

        # Verify that all generated insertions refer to gaps that exist
        for pattern in insertions.keys():
            if pattern not in gaps:
                raise PopulationError(f"An insertion was generated for a non-existing gap '{pattern}'.", template, raw_model_output)

        # Verify that all generated insertions are valid (i.e., not empty and not identical with the original gap instruction)
        for pattern in insertions.keys():
            if insertions[pattern] == None or insertions[pattern].strip() == "":
                raise PopulationError(f"Invalid insertion '{insertions[pattern]}' attempted into gap '{pattern}'. Insertion is empty.", template, raw_model_output)
            
            stripped_pattern = pattern.strip("[[").strip("]]").strip("{{").strip("}}")
            stripped_insertion = insertions[pattern].strip("[[").strip("]]").strip("{{").strip("}}")
            if stripped_insertion == stripped_pattern:
                raise PopulationError(f"Invalid insertion '{insertions[pattern]}' attempted into gap '{pattern}'. Insertion is identical with the gap instruction.", template, raw_model_output)

        return True


class TestGenerator(ABC):
    """
    Abstract base class for test generators. A test generator is responsible for generating test cases for a particular cognitive bias.
    
    Attributes:
        BIAS (str): The cognitive bias associated with this test generator.    
    """

    def __init__(self):
        self.BIAS = "None"

    @abstractmethod
    def generate_all(self, model: LLM, scenarios: list[str], temperature: float = 0.0, seed: int = 42, num_instances: int = 1, max_retries: int = 5) -> list[TestCase]:
        """
        Generates all test cases at once for the cognitive bias associated with this test generator.

        Args:
            model (LLM): The LLM model to use for generating the test case.
            scenarios (list[str]): The list of scenarios for which to generate the test case.
            temperature (float): The temperature to use for generating the test cases.
            seed (int): A seed for deterministic randomness.
            num_instances (int): The number of instances to generate for each scenario.
            max_retries (int): The maximum number of retries of generation for the bias.

        Returns:
            A list of TestCase objects representing the generated test cases.
        """
        pass
    
    @abstractmethod
    def generate(self, model: LLM, scenario: str, config_values: dict = {}, temperature: float = 0.0, seed: int = 42) -> TestCase:
        """
        Generates a test case for the cognitive bias associated with this test generator.

        Args:
            model (LLM): The LLM model to use for generating the test case.
            scenario (str): The scenario for which to generate the test case.
            config_values (dict): A dictionary containing the configuration data for the test case.
            temperature (float): The temperature to use for generating the test case.
            seed (int): A seed for deterministic randomness.

        Returns:
            A TestCase object representing the generated test case.
        """
        pass

    def load_config(self, bias: str) -> TestConfig:
        """
        Loads the test configuration from the specified XML file.

        Args:
            path (str): The path to the XML file containing the test configuration.

        Returns:
            A TestConfig object representing the loaded test configuration.
        """
        return TestConfig(f"./biases/{bias.title().replace(' ', '')}/config.xml")

    def populate(self, model: LLM, control: Template, treatment: Template, scenario: str, temperature: float = 0.0, seed: int = 42) -> tuple[Template, Template]:
        """
        Populates the control and treatment templates using the provided LLM model and scenario.

        Args:
            model (LLM): The LLM model to use for populating the templates.
            control (Template): The control template.
            treatment (Template): The treatment template.
            scenario (str): The scenario for which to populate the templates.
            temperature (float): The temperature to use for generating the test cases.
            seed (int): A seed for deterministic randomness.

        Returns:
            A tuple containing the populated control and treatment templates.
        """

        # Populate the templates using the model and scenario
        control, treatment = model.populate(control, treatment, scenario, temperature, seed)

        return control, treatment


class Metric(ABC):
    """
    Abstract base class for metrics. A metric is responsible for measuring the presence and strength of a cognitive bias in a Large Language Model (LLM).
    
    Attributes:
        BIAS (str): The cognitive bias associated with this metric.
    """

    def __init__(self):
        self.BIAS = "None"

    @abstractmethod
    def compute(self, test_results: list[tuple[TestCase, DecisionResult]]) -> float:
        pass